import re
import math
from typing import List, Dict, Optional, Tuple
from collections import Counter
from ..models.hadith import Hadith
from .hadith_service import hadith_service

class SearchService:
    def __init__(self):
        self.hadiths = hadith_service.get_all_hadiths()
        self.build_index()
    
    def build_index(self):
        """Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹"""
        print("ğŸ” Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«...")
        self.word_index = {}
        self.hadith_lengths = []
        
        for hadith in self.hadiths:
            words = self.tokenize(hadith.arabic)
            self.hadith_lengths.append(len(words))
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ ÙÙ‡Ø±Ø³ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            for word in set(words):
                if len(word) > 2:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                    if word not in self.word_index:
                        self.word_index[word] = []
                    self.word_index[word].append(hadith.id)
        
        # Ø­Ø³Ø§Ø¨ IDF (Inverse Document Frequency)
        self.idf = {}
        total_hadiths = len(self.hadiths)
        for word, hadith_ids in self.word_index.items():
            self.idf[word] = math.log(total_hadiths / (len(hadith_ids) + 1))
        
        print(f"âœ… ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø« Ø¬Ø§Ù‡Ø²: {len(self.word_index)} ÙƒÙ„Ù…Ø© ÙØ±ÙŠØ¯Ø©")
    
    def tokenize(self, text: str) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø¹ ØªØ·Ø¨ÙŠØ¹"""
        if not text:
            return []
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'[^\w\s]', '', text)
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
        words = text.split()
        return words
    
    def normalize_arabic(self, text: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø¨Ø­Ø«"""
        if not text:
            return ""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'[ÙÙ‹ÙÙŒÙÙÙ’Ù‘]', '', text)
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ù„Ù
        text = text.replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø© ÙˆØ§Ù„Ù‡Ø§Ø¡
        text = text.replace('Ø©', 'Ù‡')
        return text
    
    def simple_search(self, query: str, filters: Optional[Dict] = None) -> List[Hadith]:
        """Ø¨Ø­Ø« Ø¨Ø³ÙŠØ· (ØªØ·Ø§Ø¨Ù‚ Ø¬Ø²Ø¦ÙŠ)"""
        if not query:
            return []
        
        query_normalized = self.normalize_arabic(query)
        results = []
        
        for hadith in self.hadiths:
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ± Ø£ÙˆÙ„Ø§Ù‹
            if filters:
                if not self.apply_filters(hadith, filters):
                    continue
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
            text_normalized = self.normalize_arabic(hadith.arabic)
            if query_normalized in text_normalized:
                results.append(hadith)
                continue
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ (Ø¥Ø°Ø§ ÙˆØ¬Ø¯)
            if hadith.english_text and query.lower() in hadith.english_text.lower():
                results.append(hadith)
        
        return results
    
    def advanced_search(self, query: str, options: Optional[Dict] = None) -> Dict:
        """Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø©"""
        if not query:
            return {"results": [], "total": 0, "suggestions": []}
        
        options = options or {}
        filters = options.get('filters', {})
        page = options.get('page', 1)
        limit = options.get('limit', 20)
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        query_normalized = self.normalize_arabic(query)
        query_words = self.tokenize(query_normalized)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØµÙ„Ø© Ù„ÙƒÙ„ Ø­Ø¯ÙŠØ«
        scored_results = []
        for hadith in self.hadiths:
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±
            if filters and not self.apply_filters(hadith, filters):
                continue
            
            # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØµÙ„Ø©
            score = self.calculate_relevance(hadith, query_normalized, query_words)
            if score > 0:
                scored_results.append((hadith, score))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø©
        scored_results.sort(key=lambda x: x[1], reverse=True)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
        total = len(scored_results)
        start = (page - 1) * limit
        end = start + limit
        paginated = [item[0] for item in scored_results[start:end]]
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬
        suggestions = []
        if total == 0:
            suggestions = self.get_suggestions(query)
        
        return {
            "results": paginated,
            "total": total,
            "page": page,
            "limit": limit,
            "pages": math.ceil(total / limit) if total > 0 else 0,
            "suggestions": suggestions,
            "query": query
        }
    
    def calculate_relevance(self, hadith: Hadith, query_normalized: str, query_words: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØµÙ„Ø© Ø§Ù„Ø­Ø¯ÙŠØ« Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        score = 0.0
        text_normalized = self.normalize_arabic(hadith.arabic)
        
        # 1. ØªØ·Ø§Ø¨Ù‚ ØªØ§Ù… (Ø£Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø©)
        if query_normalized in text_normalized:
            score += 10.0
        
        # 2. ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        text_words = self.tokenize(text_normalized)
        for q_word in query_words:
            if q_word in text_words:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… IDF Ù„Ù„ÙƒÙ„Ù…Ø© (Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø© Ø£Ù‡Ù…)
                word_idf = self.idf.get(q_word, 1.0)
                score += word_idf
            elif len(q_word) > 3 and q_word[:-1] in text_words:  # ØªØ·Ø§Ø¨Ù‚ Ø¬Ø²Ø¦ÙŠ (Ø¨Ø¯ÙˆÙ† Ø¢Ø®Ø± Ø­Ø±Ù)
                score += 0.5
        
        # 3. Ù…ÙƒØ§ÙØ£Ø© Ù„Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ù‚ØµÙŠØ± (Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©)
        if len(text_words) < 20:
            score *= 1.2
        elif len(text_words) > 100:
            score *= 0.8
        
        # 4. Ù…ÙƒØ§ÙØ£Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­Ø¯ÙŠØ« ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Ø¥Ø°Ø§ ÙˆØ¬Ø¯)
        # Ù„Ø§ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ Ù‡Ù†Ø§ Ù„Ø£Ù† Ø§Ù„ÙÙ„Ø§ØªØ± ØªØ·Ø¨Ù‚ Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø³Ø§Ø¨
        
        return score
    
    def apply_filters(self, hadith: Hadith, filters: Dict) -> bool:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ± Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ÙŠØ«"""
        for key, value in filters.items():
            if key == 'book_id' and hadith.book_id != value:
                return False
            elif key == 'grade' and hadith.grade and value not in hadith.grade:
                return False
            elif key == 'narrator' and hadith.english_narrator:
                if value not in hadith.english_narrator:
                    return False
        return True
    
    def get_suggestions(self, query: str, max_suggestions: int = 5) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„Ø¨Ø­Ø« Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³"""
        query_normalized = self.normalize_arabic(query)
        if len(query_normalized) < 2:
            return []
        
        suggestions = []
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª ØªØ¨Ø¯Ø£ Ø¨Ù†ÙØ³ Ø§Ù„Ø­Ø±ÙˆÙ
        for word in self.word_index.keys():
            if word.startswith(query_normalized) and word != query_normalized:
                suggestions.append(word)
                if len(suggestions) >= max_suggestions:
                    break
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        if not suggestions:
            for word in self.word_index.keys():
                if query_normalized in word and word != query_normalized:
                    suggestions.append(word)
                    if len(suggestions) >= max_suggestions:
                        break
        
        return suggestions
    
    def get_filters_options(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ÙÙ„Ø§ØªØ± Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        books = hadith_service.get_all_books()
        grades = set()
        narrators = set()
        
        for hadith in self.hadiths[:1000]:  # Ù†Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
            if hadith.grade:
                grades.add(hadith.grade)
            if hadith.english_narrator:
                narrators.add(hadith.english_narrator)
        
        return {
            "books": [{"id": b.id, "name": b.name_ar} for b in books],
            "grades": list(grades)[:50],
            "narrators": list(narrators)[:50]
        }

# Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§Ù„Ø®Ø¯Ù…Ø©
search_service = SearchService()